[["bias-and-variance-of-penalized-regression.html", "Chapter 3 Bias and Variance of Penalized Regression 3.1 Assumptions 3.2 Bias 3.3 Variance 3.4 Mean Squared Error", " Chapter 3 Bias and Variance of Penalized Regression After introducing penalized regression models and finding the estimator for \\(\\hat{\\beta}\\), we now turn to the properties of these models. In particular, we derive the bias and variance of the estimators, given our focus on the trade-off between them, formally showing that penalized regression models are biased, but have lower variance than OLS. For simplicity, we work exclusively with ridge regression in these derivations, given that it is the only penalized regression model with a closed form solution, but numerical solutions for LASSO and elastic net show similar properties, as we show through simulation later on. 3.1 Assumptions We begin with the standard assumptions regarding our error terms: \\(E[\\epsilon \\mid X]=0\\) \\(Var[\\epsilon |X]=\\sigma^2 I_n\\) Error terms are uncorrelated 3.2 Bias First, we derive the bias of the ridge estimator. Using the fact that \\(\\hat{B}_{ridge}=(X^TX+ \\lambda I)^{-1}X^Ty\\), we have: \\[ \\begin{aligned} E\\left[\\hat{B}_{ridge}\\mid X \\right]-\\beta&amp;=E\\left[(X^TX+ \\lambda I)^{-1}X^Ty \\mid X \\right]-\\beta\\\\ &amp;= (X^TX+ \\lambda I)^{-1}X^TE\\left[X\\beta+\\epsilon \\mid X \\right]-\\beta \\text{ by the linearity of expected value}\\\\ &amp;= (X^TX+ \\lambda I)^{-1}X^TX\\beta-\\beta \\text{ since the error term has mean 0}\\\\ \\end{aligned} \\] Thus, the ridge estimator is unbiased only in the case that \\((X^TX+ \\lambda I)^{-1}X^TX=I\\), which occurs when \\(\\lambda=0\\) (that is, when we just have the OLS estimator and no penalty). When \\(\\lambda&gt;0\\), on the other hand, the ridge estimator is biased. This result is intuitive: since the OLS estimator is unbiased, we are introducing bias into our estimator by penalizing large coefficients. As we can see, the larger the penalty, the more biased our estimator will be. 3.3 Variance Now, we derive the variance of the ridge estimator. To do so, we will write the ridge estimator as a function of the OLS estimator, using the fact that \\(Var[\\hat{\\beta}_{OLS} \\mid X]= \\sigma^2 (X^T X)^{-1}\\). First, notice that we can write the \\(\\hat{B}_{ridge}\\) as a function of \\(\\hat{B}_{OLS}\\) \\[ \\begin{aligned} \\hat{B}_{ridge}&amp;=(X^TX+ \\lambda I)^{-1}X^Ty\\\\ &amp;=(X^TX+ \\lambda I)^{-1}X^TX(X^TX)^{-1}X^T y \\text{ since } X^TX(X^TX)^{-1}=I\\\\ &amp; = (X^TX+ \\lambda I)^{-1} X^TX \\hat{B}_{OLS} \\text{ since } \\hat{B}_{OLS}=(X^TX)^{-1}X^T y \\end{aligned} \\] Now, we can derive the variance, using the fact that \\(Var[\\hat{\\beta}_{OLS} \\mid X]= \\sigma^2 (X^T X)^{-1}\\). We have: \\[ \\begin{aligned} Var[\\hat{B}_{ridge} \\mid X]&amp;=Var\\left[(X^TX+ \\lambda I)^{-1} X^TX \\hat{B}_{OLS}\\right]\\\\ &amp;= (X^TX+ \\lambda I)^{-1}X^TX X^TVar[\\hat{B}_{OLS} \\mid X]\\left(\\left(X^TX+ \\lambda I\\right)^{-1}X^TX X^T\\right)^{T}\\\\ &amp;= (X^TX+ \\lambda I)^{-1}X^TX X^T\\sigma^2 (X^TX)^{-1}\\left(\\left(X^TX+ \\lambda I\\right)^{-1}X^TX X^T\\right)^{T} \\text{ since } Var[\\hat{\\beta}_{OLS} \\mid X]= \\sigma^2 (X^T X)^{-1}\\\\ &amp;= \\sigma^2 (X^TX+\\lambda I)^{-1}X^TX\\left(\\left(X^TX+ \\lambda I\\right)^{-1}\\right)^{T} \\end{aligned} \\] Now, we compare this variance to that of the OLS estimator: \\[ \\begin{aligned} Var(\\hat{\\beta}_{OLS})-Var(\\hat{\\beta}_{ridge})&amp;=\\sigma^2 (X^T X)^{-1}-\\sigma^2 (X^TX+\\lambda I)^{-1}X^TX\\left(\\left(X^TX+ \\lambda I\\right)^{-1}\\right)^{T}\\\\ &amp;= \\sigma^2 \\left((X^T X)^{-1}- (X^TX+\\lambda I)^{-1}X^TX\\left(\\left(X^TX+ \\lambda I\\right)^{-1}\\right)^{T} \\right)\\\\ &amp;= \\sigma^2 (X^TX+\\lambda I)^{-1} (2\\lambda X^TX)^{-2} +\\lambda^2 (X^TX)^{-3})(X^TX+ \\lambda I)^{-1})^{T}\\\\ &amp;= \\sigma^2 (X^TX+\\lambda I)^{-1}(2\\lambda I + \\lambda^2(X^TX))((X^TX+\\lambda I)^{_1})^T \\end{aligned} \\] Since \\((X^TX+\\lambda I)^{-1}\\), \\((2\\lambda I + \\lambda^2(X^TX))\\), and \\(((X^TX+\\lambda I)^{_1})^T\\) are all non-negative definite, this means that the variance of the OLS estimator is greater than that of the ridge estimator, as expected. 3.4 Mean Squared Error The expected error, the mean squared error, or \\(E[(y - \\hat{f}(x))^2]\\) of our predictions can also be expressed as \\[ \\begin{aligned} E[(y - \\hat{f}(x))^2] = Bias[\\hat{f}(x)]^2 + Var[\\hat{f}(x)] + \\sigma^2 \\end{aligned} \\] There are three components that affect the expected error of our predictions: Bias Variance “Irreducible Error term” \\(\\sigma^2\\) It is possible to reduce expected error by increasing bias and reducing variance by a lot. Which is why we use ridge regression and lasso to increase the bias by significantly reducing the variance. The trade-off is an important way to reduce our mean squared error for values outside our sample. We tune our \\(\\lambda\\) parameter using cross-validation and see how it reacts when there is “new” data. Proof of the Bias Variance Trade-off we are comparing our estimate of \\(\\hat{y}\\) with the true value of y. \\[ \\begin{aligned} MSE(\\hat{y}) = E[(y - \\hat{y})^2] &amp;= E(y^2 - 2y\\hat{y} + \\hat{y}^2)\\\\ &amp;= E(y^2) - 2E(y)E(y^2) + E(\\hat{y}^2) \\\\ &amp;= Var(y) + E(y)^2 - 2E(y)E(\\hat{y}) + E(\\hat{y}^2)\\\\ &amp;= Var(y) + y^2 - 2yE(\\hat{y}) + E(\\hat{y}^2) \\\\ &amp; = Var(y) + E(\\hat{y}^2)- 2yE(\\hat{y})+ y^2 \\\\ &amp;= Var(y) + (E(\\hat{y}) - y)^2 \\end{aligned} \\] "]]
