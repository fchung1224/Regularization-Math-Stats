[["index.html", "MATH 455 Regularization Preface", " MATH 455 Regularization Federico Chung, Lucas Leiter, and Aidan Toner-Rodgers Preface In our final project for MATH 455: Mathematical Statistics, we explore the theory and application of penalized regression models. "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction Linear regression is one of the most important tools in statistics, useful for both prediction and causal inference across a range of disciplines. At its core, regression is about using observed data to estimate the unknown true relationship between variables. Intuitively, this suggests two key features we might desire in a regression model: Low Bias: The model predicts the true relationship correctly on average Low Variance: The predictions do not vary widely These two features are illustrated nicely in the following figure: Ideally, a regression model would satisfy both of these conditions, and the estimates would look something like the bulls-eye on the top left. In many cases, however, there is a tradeoff between low bias and low variance. Ordinary least squares, the most commonly used regression model, does well on the first of these two criteria–under reasonable assumptions, it correctly predicts the true relationship on average. However, in cases with many explanatory variables, it fails the second criteria–even though the estimates are correct on average they may vary wildly depending on the specific sample of data that is used. In such cases, the OLS estimates will look like the bulls-eye on the top right in the figure above. In this project we consider a modification of Ordinary Least Squares, called regularization, that attempts to overcome this limitation. By penalizing large coefficients, this approach trades off unbiasedness (meaning that the model no longer makes the correct prediction on average) for less variance (meaning that a given estimate may be closer to the true value). Thus, penalized regression estimates may look more like the bulls-eye on the bottom left. We begin by introducing penalized regression models in section 2 and present derivations of their bias and variance in section 3. Next, we discuss the pros and cons of different types of penalized regression models in section 4. Then, in section 5, we illustrate the properties derived in section 3 on simulated data, showing how penalized regression estimates differ from OLS and how different types of regularization models differ from each other. Finally, in section 5, we briefly discuss interesting real world applications of penalized regression. "],["penalized-regression-estimators.html", "Chapter 2 Penalized Regression Estimators 2.1 Penalized Regression Models 2.2 Deriving the Ridge Estimator", " Chapter 2 Penalized Regression Estimators This section introduces penalized regression models and then derives the estimator for ridge regression. 2.1 Penalized Regression Models In any type of linear regression, we assume that the relationship between \\(y\\), our dependent variable, and \\(X\\), a matrix of our independent variables, can be written as: \\[y_i=\\beta^TX+\\epsilon_i\\] where \\(\\beta\\) is a vector of coefficients and \\(\\epsilon_i\\) is the random error component with mean zero. The goal is to estimate the true coefficient values of \\(\\beta\\). As seen in Stat 155, OLS regression estimates \\(beta\\) by minimizing the sum of squared errors: \\[\\hat{\\beta}_{OLS}=\\text{argmin}_\\beta \\frac{1}{N}\\sum_{i=1}^n (y_i-\\beta^TX_i)^2.\\] In penalized regression, we modify this process by adding a penalty term for large coefficient values. There are two common ways to do this. The first, called ridge regression, estimates \\(\\beta\\) by minimizing the following equation: \\[\\hat{\\beta}_{Ridge}=\\text{argmin}_\\beta \\frac{1}{N}\\sum_{i=1}^n (y_i-\\beta^TX_i)^2+\\lambda||\\beta||_2^2\\] where \\(||\\beta||_2^2=\\sum_j B_j^2\\) and the penalty is thus based on the squared 2-norm of the coefficient vector. \\(\\lambda\\) represents the penalty parameter – a larger value of \\(\\lambda\\) means that we penalized large coefficients more, while a \\(\\lambda\\) value of zero is just OLS. The second common method is called LASSO regression, and uses the following equation instead: \\[\\hat{\\beta}_{LASSO}=\\text{argmin}_\\beta \\frac{1}{N}\\sum_{i=1}^n (y_i-\\beta^TX_i)^2+\\lambda||\\beta||_1\\] where \\(||\\beta||_1\\sum_{j \\in J}|B_j|\\). In LASSO, the penalty is based on the 1-norm of the coefficient vector, rather than the 2-norm as in ridge. Again, \\(\\lambda\\) represents the penalization parameter, with larger values corresponding to more deviation from OLS. While both LASSO and Ridge have pros and cons (as we show through simulation later on) only Ridge regression has a closed form solution. Thus, our theoretical derivations focus only on ridge regression, although the concepts apply to LASSO a well. 2.2 Deriving the Ridge Estimator Now, after inroducing the idea behind penalized regression models, we derive the estimator for ride regression. Assume that we have predictors that range from \\(\\bar{x}^T_i = 1,x^2_i,x^3_i,...,x_i^n\\) \\(\\beta\\) includes all the coefficients for all our predictors. \\[ \\begin{aligned} \\beta^T = [\\beta_0, \\beta_1,..., \\beta_n] \\\\ J = \\sum^m_{i=0} (x_i^T\\beta - y_i)^2 + \\lambda ||\\beta||^2 \\end{aligned} \\] J is our cost function, the value that we are trying to minimize. Normally we only reduce the sum of squared residuals $^m_{i=0} (x_i^T- y_i)^2 $ but now we also include our Shrinkage parameter \\(\\lambda||\\beta||^2\\), or also called \\(L_2\\) regularization \\[ \\begin{aligned} X^T = [\\bar{X}^T_1, \\bar{X}^T_2, \\bar{X}^T_3, \\bar{X}^T_4, ...,\\bar{X}^T_n] \\\\ Y^T = [y_1, y_2, y_3,y_4,...,y_n] \\\\ J = (X\\beta - Y)^T(X\\beta - Y) + \\lambda \\beta^T\\beta \\\\ \\end{aligned} \\] \\((X\\beta - Y)^T(X\\beta - Y)\\) –&gt; a way to express squared values in linear algebra lingo We try to minimize J so we use its derivative and set it to zero to see what value of \\(\\beta\\) will minimize the new cost function J: \\[ \\begin{aligned} \\frac{dJ}{d\\beta} = 2X^T(X\\beta - Y)+ \\lambda(2\\beta)\\\\ 2X^T(X\\beta - Y)+ \\lambda(2\\beta) \\overset{set}{=} 0 \\\\ X^T(X\\beta - Y)+ \\lambda(\\beta) = 0 \\\\ X^TX\\beta - X^TY + \\lambda\\beta = 0 \\\\ X^TX\\beta + \\lambda\\beta = X^TY\\\\ (X^TX+ \\lambda I)\\beta = X^TY\\\\ \\beta = (X^TX+ \\lambda I)^{-1}X^TY \\\\ \\end{aligned} \\] If \\(\\lambda = 0\\) we have a normal OLS model where: \\[ \\beta = (X^TX)^{-1}X^TY \\\\ \\] Thus, ridge estimates are a scaled version of the least squares estimates where \\(\\hat{\\beta}^{ridge} = \\hat{\\beta}/(1+\\lambda)\\) "],["bias-and-variance-of-penalized-regression.html", "Chapter 3 Bias and Variance of Penalized Regression 3.1 Assumptions 3.2 Bias 3.3 Variance 3.4 Mean Squared Error", " Chapter 3 Bias and Variance of Penalized Regression After introducing penalized regression models and finding the estimator for \\(\\hat{\\beta}\\), we now turn to the properties of these models. In particular, we derive the bias and variance of the estimators, given our focus on the trade-off between them, formally showing that penalized regression models are biased, but have lower variance than OLS. For simplicity, we work exclusively with ridge regression in these derivations, given that it is the only penalized regression model with a closed form solution, but numerical solutions for LASSO and elastic net show similar properties, as we show through simulation later on. 3.1 Assumptions We begin with the standard assumptions regarding our error terms: \\(E[\\epsilon \\mid X]=0\\) \\(Var[\\epsilon |X]=\\sigma^2 I_n\\) Error terms are uncorrelated 3.2 Bias First, we derive the bias of the ridge estimator. Using the fact that \\(\\hat{B}_{ridge}=(X^TX+ \\lambda I)^{-1}X^Ty\\), we have: \\[ \\begin{aligned} E\\left[\\hat{B}_{ridge}\\mid X \\right]-\\beta&amp;=E\\left[(X^TX+ \\lambda I)^{-1}X^Ty \\mid X \\right]-\\beta\\\\ &amp;= (X^TX+ \\lambda I)^{-1}X^TE\\left[X\\beta+\\epsilon \\mid X \\right]-\\beta \\text{ by the linearity of expected value}\\\\ &amp;= (X^TX+ \\lambda I)^{-1}X^TX\\beta-\\beta \\text{ since the error term has mean 0}\\\\ \\end{aligned} \\] Thus, the ridge estimator is unbiased only in the case that \\((X^TX+ \\lambda I)^{-1}X^TX=I\\), which occurs when \\(\\lambda=0\\) (that is, when we just have the OLS estimator and no penalty). When \\(\\lambda&gt;0\\), on the other hand, the ridge estimator is biased. This result is intuitive: since the OLS estimator is unbiased, we are introducing bias into our estimator by penalizing large coefficients. As we can see, the larger the penalty, the more biased our estimator will be. 3.3 Variance Now, we derive the variance of the ridge estimator. To do so, we will write the ridge estimator as a function of the OLS estimator, using the fact that \\(Var[\\hat{\\beta}_{OLS} \\mid X]= \\sigma^2 (X^T X)^{-1}\\). First, notice that we can write the \\(\\hat{B}_{ridge}\\) as a function of \\(\\hat{B}_{OLS}\\) \\[ \\begin{aligned} \\hat{B}_{ridge}&amp;=(X^TX+ \\lambda I)^{-1}X^Ty\\\\ &amp;=(X^TX+ \\lambda I)^{-1}X^TX(X^TX)^{-1}X^T y \\text{ since } X^TX(X^TX)^{-1}=I\\\\ &amp; = (X^TX+ \\lambda I)^{-1} X^TX \\hat{B}_{OLS} \\text{ since } \\hat{B}_{OLS}=(X^TX)^{-1}X^T y \\end{aligned} \\] Now, we can derive the variance, using the fact that \\(Var[\\hat{\\beta}_{OLS} \\mid X]= \\sigma^2 (X^T X)^{-1}\\). We have: \\[ \\begin{aligned} Var[\\hat{B}_{ridge} \\mid X]&amp;=Var\\left[(X^TX+ \\lambda I)^{-1} X^TX \\hat{B}_{OLS}\\right]\\\\ &amp;= (X^TX+ \\lambda I)^{-1}X^TX X^TVar[\\hat{B}_{OLS} \\mid X]\\left(\\left(X^TX+ \\lambda I\\right)^{-1}X^TX X^T\\right)^{T}\\\\ &amp;= (X^TX+ \\lambda I)^{-1}X^TX X^T\\sigma^2 (X^TX)^{-1}\\left(\\left(X^TX+ \\lambda I\\right)^{-1}X^TX X^T\\right)^{T} \\text{ since } Var[\\hat{\\beta}_{OLS} \\mid X]= \\sigma^2 (X^T X)^{-1}\\\\ &amp;= \\sigma^2 (X^TX+\\lambda I)^{-1}X^TX\\left(\\left(X^TX+ \\lambda I\\right)^{-1}\\right)^{T} \\end{aligned} \\] Now, we compare this variance to that of the OLS estimator: \\[ \\begin{aligned} Var(\\hat{\\beta}_{OLS})-Var(\\hat{\\beta}_{ridge})&amp;=\\sigma^2 (X^T X)^{-1}-\\sigma^2 (X^TX+\\lambda I)^{-1}X^TX\\left(\\left(X^TX+ \\lambda I\\right)^{-1}\\right)^{T}\\\\ &amp;= \\sigma^2 \\left((X^T X)^{-1}- (X^TX+\\lambda I)^{-1}X^TX\\left(\\left(X^TX+ \\lambda I\\right)^{-1}\\right)^{T} \\right)\\\\ &amp;= \\sigma^2 (X^TX+\\lambda I)^{-1} (2\\lambda X^TX)^{-2} +\\lambda^2 (X^TX)^{-3})(X^TX+ \\lambda I)^{-1})^{T}\\\\ &amp;= \\sigma^2 (X^TX+\\lambda I)^{-1}(2\\lambda I + \\lambda^2(X^TX))((X^TX+\\lambda I)^{_1})^T \\end{aligned} \\] Since \\((X^TX+\\lambda I)^{-1}\\), \\((2\\lambda I + \\lambda^2(X^TX))\\), and \\(((X^TX+\\lambda I)^{_1})^T\\) are all non-negative definite, this means that the variance of the OLS estimator is greater than that of the ridge estimator, as expected. 3.4 Mean Squared Error A nice way to measure the overall performance of an estimator is with the mean squared error, which is a measure of how much the model’s prediction differ from the true value. Specifically, the mean squared error is written as \\[MSE=E[(y - \\hat{f}(x))^2]\\] Since the MSE depends on the square of the difference between the predicted and true value, it is a function of both the bias and the variance. Specifically, we can express the MSE alternatively as: \\[ \\begin{aligned} MSE(\\hat{y}) = E[(y - \\hat{y})^2] &amp;= E(y^2 - 2y\\hat{y} + \\hat{y}^2)\\\\ &amp;= E(y^2) - 2E(y)E(y^2) + E(\\hat{y}^2) \\\\ &amp;= Var(y) + E(y)^2 - 2E(y)E(\\hat{y}) + E(\\hat{y}^2)\\\\ &amp;= Var(y) + y^2 - 2yE(\\hat{y}) + E(\\hat{y}^2) \\\\ &amp; = Var(y) + E(\\hat{y}^2)- 2yE(\\hat{y})+ y^2 \\\\ &amp;= Var(y) + (E(\\hat{y}) - y)^2 \\end{aligned} \\] Thus, there are three components that affect the expected error of our predictions: Bias Variance “Irreducible Error term” \\(\\sigma^2\\) From this decomposition, we begin to see how penalized regression can have improved performance over OLS. By sacrificing higher bias in order to get much lower variance, the MSE of penalized regression can potentially be much lower in certain circumstances, particularly when there are many covariates. "],["simulation.html", "Chapter 4 Simulation 4.1 Calculate Bias, Variance, MSE of the coefficients in the model 4.2 Results 4.3 Visualizations", " Chapter 4 Simulation In this simulation we will use 4 different models and test how the coefficient estimates for each model varies from each other. Each of the models tested, outside of OLS, have a loss function that includes additional “penalties”, which is why these functions are labeled as penalized regression models. The models we will be testing are the following: -OLS Which tries to find \\(\\beta\\) estimates that minimize: \\[ \\begin{aligned} MSE = \\frac{1}{N}(Y - X\\beta)^T(Y-X\\beta) \\end{aligned} \\] -Ridge Regression Which has an additional regression penalty, called L2: \\(\\lambda \\beta^T\\beta\\) Ridge Regression tries to find \\(\\beta\\) estimates that minimizes: \\[ \\begin{aligned} &amp;= MSE + \\lambda \\beta^T\\beta\\\\ &amp;= \\frac{1}{N}(Y - X\\beta)^T(Y-X\\beta) + \\lambda \\beta^T\\beta \\end{aligned} \\] -LASSO Which has an additional regression penalty, called L1: \\(\\lambda|\\beta|\\) LASSO tries to find \\(\\beta\\) estimates that minimizes: \\[ \\begin{aligned} &amp;= MSE + \\lambda|\\beta|\\\\ &amp;= \\frac{1}{N}(Y - X\\beta)^T(Y-X\\beta) + \\lambda|\\beta| \\end{aligned} \\] -Elastic Net Which is a combination of both L1 and L2 penalties: $^T+|| $ Elastic Net Regression tries to find \\(\\beta\\) estimates that minimizes: \\[ \\begin{aligned} &amp;= MSE + \\lambda \\beta^T\\beta+\\lambda|\\beta|\\\\ &amp;= \\frac{1}{N}(Y - X\\beta)^T(Y-X\\beta) + \\lambda_1 \\beta^T\\beta+\\lambda_2|\\beta| \\end{aligned} \\] ## Simulation of explanatory variables We first start by creating a simulation of data. The dataset includes a covariance matrix to include multicollinearity within the columns of the data. This allows us to explore the benefits of penalized regression models when there is multicollinearity. Based on a pre-determined set of betas we use the these values to create our dependent variable for which we add some noise e. We decided to use a small dataset with loads of predictors to see what potential benefits Penalized Regression Models can have on small datasets. The true values of Y will be estimated using the following formula: \\[ Y = X\\beta + \\epsilon \\] Where \\(\\epsilon \\sim N(0,25)\\) We simulated the dataset for 100 times. And the dataset simulation_all contains all the 50 different coefficient estimates for all the 100 iterations for all 4 models. simulation_all&lt;-read_csv(&quot;simulation_all.csv&quot;) ## Warning: Missing column names filled in: &#39;X1&#39; [1] ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_double(), ## Model = col_character() ## ) ## ℹ Use `spec()` for the full column specifications. Each simulation consists of 50 predictors, and 100 observations. 4.1 Calculate Bias, Variance, MSE of the coefficients in the model bias&lt;-simulation_all%&gt;% mutate(Coefficient_Means = rowMeans(.[4:103]))%&gt;% mutate(Bias = Coefficient_Means - betas)%&gt;% dplyr::select(c(Model, Coefficient, Bias, Coefficient_Means, betas)) simulation_all_long&lt;- gather(simulation_all, Simulation, Estimate, simulation_1:simulation_100) MSE&lt;-simulation_all_long%&gt;% mutate(diff = (Estimate - betas)^2)%&gt;% group_by(Model, Coefficient)%&gt;% summarize(MSE = mean(diff)) ## `summarise()` regrouping output by &#39;Model&#39; (override with `.groups` argument) var&lt;-simulation_all_long%&gt;% group_by(Model, Coefficient)%&gt;% mutate(mean_Estimate = mean(Estimate))%&gt;% mutate(diff = (Estimate - mean(Estimate))^2)%&gt;% group_by(Model, Coefficient)%&gt;% summarize(Var = mean(diff)) ## `summarise()` regrouping output by &#39;Model&#39; (override with `.groups` argument) results&lt;-bias%&gt;% left_join(MSE)%&gt;% left_join(var) ## Joining, by = c(&quot;Model&quot;, &quot;Coefficient&quot;) ## Joining, by = c(&quot;Model&quot;, &quot;Coefficient&quot;) results_df&lt;-results%&gt;% dplyr::select(Model, Coefficient, betas, Coefficient_Means, Bias, Var, MSE)%&gt;% mutate(Bias_2_plus_Var = Var + Bias^2) colnames(results_df)&lt;-c(&quot;Model&quot;, &quot;Coefficient #&quot;,&quot;True Coefficient Values&quot;, &quot;Estimation Mean&quot;, &quot;Bias&quot;,&quot;Var&quot;, &quot;MSE&quot;, &quot;Calculated MSE&quot;) results_df%&gt;% filter(`Coefficient #` ==4) ## # A tibble: 4 x 8 ## Model `Coefficient #` `True Coefficie… `Estimation Mea… Bias Var MSE ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 OLS 4 -0.5 -0.447 0.0525 0.0625 0.0626 ## 2 RIDGE 4 -0.5 -0.367 0.133 0.0362 0.0437 ## 3 LASSO 4 -0.5 -0.396 0.104 0.0600 0.0633 ## 4 ELAS… 4 -0.5 -0.378 0.122 0.0421 0.0478 ## # … with 1 more variable: `Calculated MSE` &lt;dbl&gt; We decided to look how our Models fair for the Coefficient of X4, as we can see most of the models come close to the True Coefficient Value of -0.5. And similar to the proofs we solved in previous sections, we see that OLS has the lowest coefficient bias for X4 but unfortunately it has the greatest Variance. Because of its high variance it ends up having the second largest MSE even though it had the lowest Bias. As we can see the property that \\(MSE = Bias^2 + Var\\) is seen by how close the value of the Calcualted MSE is to the actual MSE value. And in terms of what it means when evaluating which model to use, by increasing the bias like in the RIDGE Model we are able to reduce the MSE value by almost 0.02. Lets see what the average and median value of Bias, MSE, and Variance for our coefficients fair for all the models: 4.2 Results #lets look at coefficient 1 and how the bias, variance and MSE have changed median_results&lt;-results%&gt;% mutate(Bias_2 = Bias^2)%&gt;% dplyr::select(c(Bias_2, Var, MSE, Model))%&gt;% group_by(Model)%&gt;% summarize_if(is.numeric, median, na.rm=TRUE) average_results&lt;-results%&gt;% mutate(Bias_2 = Bias^2)%&gt;% dplyr::select(c(Bias_2, Var, MSE, Model))%&gt;% group_by(Model)%&gt;% summarize_if(is.numeric, mean, na.rm=TRUE) average_results ## # A tibble: 4 x 4 ## Model Bias_2 Var MSE ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ELASTIC NET 0.691 0.0410 0.646 ## 2 LASSO 0.732 0.0566 0.703 ## 3 OLS 0.828 0.0642 0.808 ## 4 RIDGE 0.667 0.0354 0.616 Models&lt;- c(&quot;OLS&quot;,&quot;LASSO&quot;,&quot;ELASTIC NET&quot;,&quot;RIDGE&quot;) average_results%&gt;% gather(Estimate_Name, Estimate, Bias_2:MSE)%&gt;% ggplot(aes(x = Model, y = Estimate , fill = Estimate_Name))+ geom_bar(stat = &quot;identity&quot;, position = position_dodge())+ scale_x_discrete(limits = Models) As it can be seen by the results above, we see that on average, Ridge regression tends to have the lowest MSE estimates, while OLS has the highest MSE estimates. This is a bit expected given the multicolinearity of the dataset for which some of the beta values were zero. Unexpectedly, OLS has the highest out of all the models median_results ## # A tibble: 4 x 4 ## Model Bias_2 Var MSE ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ELASTIC NET 0.516 0.0386 0.368 ## 2 LASSO 0.524 0.0565 0.425 ## 3 OLS 0.551 0.0627 0.526 ## 4 RIDGE 0.500 0.0341 0.338 median_results%&gt;% gather(Estimate_Name, Estimate, Bias_2:MSE)%&gt;% ggplot(aes(x = Model, y = Estimate , fill = Estimate_Name))+ geom_bar(stat = &quot;identity&quot;, position = position_dodge())+ scale_x_discrete(limits = Models) Similar to the average values, we still see the same pattern. Ridge has both the lowest Bias and Variance, and therefore lowest MSE. And OLS has the highest Bias and Variance and thus the highest MSE. tuning_all&lt;-read_csv(&quot;tuning_all.csv&quot;) ## Warning: Missing column names filled in: &#39;X1&#39; [1] ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## X1 = col_double(), ## alpha = col_double(), ## lambda = col_double(), ## Model = col_character(), ## Simulation_number = col_double() ## ) 4.3 Visualizations tuning_all%&gt;% group_by(Model)%&gt;% summarise(mean_lambda = mean(lambda), mean_alpha = mean(alpha)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 3 x 3 ## Model mean_lambda mean_alpha ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 ELASTIC NET 0.111 0.033 ## 2 LASSO 0.0127 1 ## 3 RIDGE 0.136 0 As we can see from this table, ELASTIC NET prefers to have a smaller alpha value, meaning that the ELASTIC NET will have a higher Ridge or L2 weight. This makes sense as compared to LASSO, the RIDGE regression provides results with lower MSE. Also as seem by the graph below, we see that the estimates for alpha for the elastic net have modes around 0 and 0.1. tuning_all%&gt;% filter(Model == &quot;ELASTIC NET&quot;)%&gt;% ggplot()+ geom_density(aes(x = alpha)) tuning_all%&gt;% ggplot()+ geom_density(aes(x = lambda))+ facet_grid(~Model) The tuning parameters are very interesting to explore. Firstly we see that the values of lambda seem to be centered around one value. LASSO seems to have a very clear estimate across simulations, while ridge and elastic net follow sparser values for lambda. When comparing Ridge and Elastic Net estimates for lambda we see that Elastic Net seems to be more smooth bi-modal distribution. Ridge regression seems to be a multi-modal distribution that is not as smooth. "],["applications.html", "Chapter 5 Applications 5.1 Deep Learning Algorithms 5.2 Autonomous Vehicles 5.3 Fraud Detection", " Chapter 5 Applications 5.1 Deep Learning Algorithms Deep learning is a machine learning technique that attempts to mimic the workings of the human brain in processing data for use in detecting objects, recognizing speech, translating languages, and making decisions. Overall, its goal is to teach computer what comes naturally to humans: learning by example. This requires the machine to “learn” rather than “memorize”. In other words, a machine that is given training data ideally should be able to pick up on the patterns on that data without overfitting to the “noise” in the data so that the model can be successful when confronted with brand new data. Let’s look at the model on the right side of the above figure. This model fits extremely well to the data. However, it does not closely conform to the true function. Thus, the model is overfitted; it has high complexity, and picks up and “memorizes” all of the noise and random fluctuation in the data. However, if given new data derived from the same function, the model will perform very poorly, as it has not attempted to recognize the true function that describes the data. In contrast, the model on the left hand side of the above figure does not overfit to the data. Although the model has slightly more bias to that specefic sample of data than does the model on the right hand side, it will perform much better when faced with unseen data. It has sacrificed a little bit of present accuracy in order to learn the general pattern of the data. 5.2 Autonomous Vehicles In the context of deep learning, the utilization of regularization has massive implications. Let’s take autonomous vehicles as an example. Autonomous vehicles use deep learning algorithms to detect various objects such as traffic signs, pedestrians, and other vehicles. Since no two of these objects that a vehicle will encounter will be exactly the same, the object detection mechanisms utilized by autonomous vehicles must use a form of regularization to ensure the safety of its passengers. A self-driving car, for example, cannot simply stop at only the stop signs it has encountered before, it must be able to recognize all stop signs, including those that may have physical defects. That is why self-driving cars use techniques such as Scale-invariant feature transform (SIFT), a feature detection algorithm that identifies local, general features in images. 5.3 Fraud Detection One of the main challenges in financial fraud detection is that only a very small percentage (much less than 1%) of transactions is fraud. As a consequence, it is difficult to learn how to identify fraud cases with high accuracy while maintaining low false positive rate. Additionally, there are various forms of froud that are continuously changing; Therefore, historical databases may lack the new fraud patterns. A solution to this that utilizes the ideas of regularization is called a “robust deep auto-encoder.” This method is made to handle training data that are not clean, meaning they may contain abnormal or outlier samples. This manifests itself well to fraud detection since financial intermediaries will face many unusual purchases that usually are not fraudulent. The method works by splitting the input data X into 2 parts: X=L+S. L is efficiently reconstructed by auto-encoder and S models the noise and outlier component. Then, a minimizing regularized objective function, or loss function, is applied to both the parts. "],["conclusion.html", "Chapter 6 Conclusion", " Chapter 6 Conclusion In this project, we consider the theory and application of penalized regression models. We motivate our study by the bias-variance traeoff– in some cases, it may be preferable to sacrifice a degree of unbiasedness in order to have an estimator with less variance. We begin by introducing penalized regression models and deriving the ridge regression estimator. Next, we prove analytically that penalized regression models are biased, but have less variance than OLS regression. We complement this theoretical work by simulation, showing empirically that penalized regression models can have lower mean squared error than OLS. Finally, we consider several interesting real worl applications of penalized regression. Given our simulation values, we are provided with very interesting findings. When we have multicolinear datasets, we see that OLS will provide the highest Coefficient Bias which is unlike what we had expected. From our simulations, we found that for our simulation specifications, Ridge Regression seemed to have the lowest bias, variance, and mse. Although it is hard to pinpoint to one specific reason, we are certain that given the multicolinearity between the X values, penalized regression models that shrink coefficients when there is multicolinearity help in reducing the bias and variance of the estimates. Initally based on the fact that we had some betas to equal zero, we expected LASSO regression to be the best in reducing the MSE values. But because there are a high number of significant parameters, as only 2 parameters are zero, Ridge Regression works better as most predictors impact the response. "],["references.html", "Chapter 7 References", " Chapter 7 References “Lasso vs Ridge vs Elastic Net | ML”. 2020. Geeks for Geeks. Liu, Hanzhong and Yuehan Yang. 2018. “Penalized regression adjusted causal effect estimates in high dimensional randomized experiments.” Working Paper. Yen-Chi Chen. 2018. “Regression: Penalized Approach.” Washington University. Steorts, Rebecca. 2015. “The Bayesian Lasso.” Duke University. Casella, George, Malay Ghosh, Jeff Gill, Minjung Kyung. “Penalized regression, standard errors, and Bayesian lassos.” Project Euclid. Regularization: Ridge Regression and the LASSO “Regularization: Ridge Regression and the LASSO.” 2006. Stanford. "],["appendix.html", "Chapter 8 Appendix 8.1 Simulation Procedure 8.2 TUNING PARAMETERS", " Chapter 8 Appendix 8.1 Simulation Procedure 8.1.1 Specify the TRUE Beta Values We create a predetermined set of \\(\\beta\\): set.seed(2) betas&lt;- (sample(-10:10,size=50,replace=TRUE))/10 8.1.2 CREATE THE SIMULATION VALUES We then use the MVR Norm function which allows us to create X variables with a specified Covarience Matrix. This allows our X-values to be Normally Distributed explanatory variables Multicollinear between the explanatory variables The reason for the use of this simulation is that we wanted to simulate real world data where we tend to experience mutlicollinearity between the explanatory variables. Also we wanted to see if including multicollinearity our coefficient estimates for ridge, lasso, or elastic net can become an improvement to the estimates of the OLS model. simulation&lt;- function(seed, betas){ set.seed(seed) n &lt;- 100 # Number of observations p &lt;- 50 # Number of predictors included in model CovMatrix &lt;- outer(1:p, 1:p, function(x,y) {.7^abs(x-y)}) x &lt;- mvrnorm(n, rep(0,p), CovMatrix) e &lt;- rnorm(n, 0, sd = 1) y &lt;- x %*% betas + e df&lt;- data.frame(y,x) return(df) } As it can be seen by the code chunk above, CovMatrix is where we specify the covariance matrix where we show that there is multicollinearity between the explanatory variables. 8.1.3 CREATING MODEL FUNCTIONS We create getters for both the model, coefficient, and tuning parameters models for the following models: OLS ## OLS get_ols_model&lt;- function(df){ set.seed(45) ols_model&lt;-train( y~.-1, data=df, method=&quot;lm&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10), preProcess = c(&quot;center&quot;, &quot;scale&quot;), metric = &quot;RMSE&quot; ) return(ols_model) } get_ols_coefficient&lt;- function(ols_model){ b_ols&lt;-coef(ols_model$finalModel) b_ols&lt;- as.data.frame(as.matrix(b_ols))%&gt;% mutate(Model= &quot;OLS&quot;,Coefficient = row_number()) return(b_ols) } RIDGE # RIDGE get_ridge_model&lt;- function(df, lambda_grid){ set.seed(45) ridge_model&lt;-train( y~.-1, data = df, method = &quot;glmnet&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10, selectionFunction = &quot;best&quot;), preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = data.frame(alpha = 0, lambda = lambda_grid), metric = &quot;RMSE&quot; ) return(ridge_model) } get_ridge_coefficient&lt;- function(ridge_model){ b_ridge&lt;- coef(ridge_model$finalModel, ridge_model$bestTune$lambda) b_ridge&lt;- as.data.frame(as.matrix(b_ridge))%&gt;% mutate(Model = &quot;RIDGE&quot;,Coefficient = row_number()) return(b_ridge) } get_ridge_tune&lt;- function(ridge_model){ ridge_tune&lt;-ridge_model$bestTune%&gt;% mutate(Model = &quot;RIDGE&quot;) return(ridge_tune) } LASSO #LASSO get_lasso_model&lt;- function(df, lambda_grid){ set.seed(45) lasso_model &lt;- train( y~.-1, data = df, method = &quot;glmnet&quot;, preProcess = c(&quot;center&quot;,&quot;scale&quot;), trControl = trainControl(method = &quot;cv&quot;, number = 10, selectionFunction = &quot;best&quot;), tuneGrid = data.frame(alpha = 1, lambda = lambda_grid), metric = &quot;RMSE&quot; ) return(lasso_model) } get_lasso_coefficient&lt;- function(lasso_model){ b_lasso&lt;- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) b_lasso&lt;- as.data.frame(as.matrix(b_lasso))%&gt;% mutate(Model = &quot;LASSO&quot;,Coefficient = row_number()) return(b_lasso) } get_lasso_tune&lt;- function(lasso_model){ lasso_tune&lt;-lasso_model$bestTune%&gt;% mutate(Model = &quot;LASSO&quot;) return(lasso_tune) } ELASTIC NET #ELASTIC NET get_elastic_net_model &lt;- function(df, srchGrd){ set.seed(45) elastic_net_model&lt;-train( y~.-1, data = df, method = &quot;glmnet&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10, selectionFunction = &quot;best&quot;), tuneGrid = srchGrd, preProcess = c(&quot;center&quot;,&quot;scale&quot;), metric = &quot;RMSE&quot; ) return(elastic_net_model) } get_elastic_net_coefficient&lt;- function(elastic_net_model){ b_elastic_net&lt;- coef(elastic_net_model$finalModel, elastic_net_model$bestTune$lambda) b_elastic_net&lt;- as.data.frame(as.matrix(b_elastic_net))%&gt;% mutate(Model = &quot;ELASTIC NET&quot;, Coefficient = row_number()) return(b_elastic_net) } get_elastic_net_tune&lt;- function(elastic_net_model){ elastic_net_tune&lt;-elastic_net_model$bestTune%&gt;% mutate(Model = &quot;ELASTIC NET&quot;) return(elastic_net_tune) } 8.1.4 TRYOUT THE SIMULATION FOR ONE ITERATION #one simulation tryout #inputs in the simulation,betas, b_final df&lt;-simulation(2,betas) lambda_grid&lt;- lambda_grid&lt;-10^seq(-3, 3, length.out = 50) alpha_grid &lt;- c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1) srchGrd = expand.grid(.alpha = alpha_grid, .lambda = lambda_grid) ols_model&lt;- get_ols_model(df) ridge_model&lt;- get_ridge_model(df, lambda_grid) lasso_model&lt;- get_lasso_model(df, lambda_grid) elastic_net_model&lt;- get_elastic_net_model(df, srchGrd) b_ols&lt;-get_ols_coefficient(ols_model) colnames(b_ols)[1]&lt;-1 b_ridge&lt;-get_ridge_coefficient(ridge_model) b_lasso&lt;-get_lasso_coefficient(lasso_model) b_elastic_net&lt;- get_elastic_net_coefficient(elastic_net_model) coefficients_all&lt;-rbind(b_ols,b_ridge,b_lasso,b_elastic_net) coefficient_column_name&lt;-paste(&quot;simulation_&quot;,i, sep = &quot;&quot;) colnames(coefficients_all)[1]&lt;- coefficient_column_name b_final_tryout&lt;-b_final%&gt;% left_join(coefficients_all) 8.1.5 SIMULATION FOR MULTIPLE ITERATIONS simulation_final_all_betas&lt;-function(iterations, betas, b_final){ lambda_grid&lt;- lambda_grid&lt;-10^seq(-3, 3, length.out = 50) alpha_grid &lt;- c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1) srchGrd = expand.grid(.alpha = alpha_grid, .lambda = lambda_grid) for (i in seq(1, iterations, 1)){ df&lt;-simulation(i,betas) ols_model&lt;- get_ols_model(df) ridge_model&lt;- get_ridge_model(df, lambda_grid) lasso_model&lt;- get_lasso_model(df, lambda_grid) elastic_net_model&lt;- get_elastic_net_model(df, srchGrd) b_ols&lt;-get_ols_coefficient(ols_model) colnames(b_ols)[1]&lt;-1 b_ridge&lt;-get_ridge_coefficient(ridge_model) b_lasso&lt;-get_lasso_coefficient(lasso_model) b_elastic_net&lt;- get_elastic_net_coefficient(elastic_net_model) b_all&lt;-rbind(b_ols,b_ridge,b_lasso,b_elastic_net) coefficient_column_name&lt;-paste(&quot;simulation_&quot;,i, sep = &quot;&quot;) colnames(b_all)[1]&lt;- coefficient_column_name b_final&lt;-b_final%&gt;% left_join(b_all) } return(b_final) } For this study we decided to use 100 iterations, if we had more time we could have used more iterations. You could change the 100 to any number of iterations you would like, and the simulation will output all the coefficient values for 100 iterations for each of the 4 models. simulation_all&lt;-simulation_final_all_betas(100, betas, b_final) #create dataset for betas write.csv(simulation_all, &quot;simulation_all.csv&quot;) 8.2 TUNING PARAMETERS tuning_final&lt;-data_frame(matrix(ncol = 3, nrow = 0)) ## Warning: `data_frame()` is deprecated as of tibble 1.1.0. ## Please use `tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. colnames(tuning_final)&lt;-c(&quot;alpha&quot;,&quot;lambda&quot;,&quot;Model&quot;) tuning_final ## # A tibble: 0 x 1 ## # … with 3 variables: alpha[,1] &lt;lgl&gt;, [,2] &lt;lgl&gt;, [,3] &lt;lgl&gt; simulation_final_tuning_parameters&lt;-function(iterations, betas, tuning_final){ for (i in seq(1, iterations, 1)){ lambda_grid&lt;- lambda_grid&lt;-10^seq(-3, 3, length.out = 50) alpha_grid &lt;- c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1) srchGrd = expand.grid(.alpha = alpha_grid, .lambda = lambda_grid) df&lt;-simulation(seed = i,betas) ridge_model&lt;- get_ridge_model(df, lambda_grid) lasso_model&lt;- get_lasso_model(df, lambda_grid) elastic_net_model&lt;- get_elastic_net_model(df, srchGrd) ridge_tune&lt;-get_ridge_tune(ridge_model) lasso_tune&lt;-get_lasso_tune(lasso_model) elastic_net_tune&lt;- get_elastic_net_tune(elastic_net_model) head(ridge_tune) head(lasso_tune) head(elastic_net_tune) tune_all&lt;-rbind(ridge_tune,lasso_tune,elastic_net_tune)%&gt;% mutate(Simulation_number = i) tuning_final&lt;- rbind(tuning_final, tune_all) } return(tuning_final) } tuning_all&lt;-simulation_final_tuning_parameters(100, betas, tuning_final) write.csv(tuning_all, &quot;tuning_all.csv&quot;) "]]
