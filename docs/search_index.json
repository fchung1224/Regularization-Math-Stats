[["index.html", "MATH 455 Regularization Preface", " MATH 455 Regularization Federico Chung, Aidan, and Lucas, Fall 2020 Macalester College Preface For our project, we’re planning to explore penalized regression models (i.e. ridge regression, LASSO, bayesian LASSO, elastic net, etc.). These methods fit in perfectly with what we’ve been learning in class regarding the bias and variance of estimators. The key idea behind penalized regression models is that in settings with many covariates, it can make sense to use estimators with more bias in order to get less variance. To do this, penalized regression models basically encourage the coefficients to be smaller (or even zero with some methods). We were especially interested in this topic because of its connection to both theory and practice. Understanding the intuition and mechanics behind penalized regression models depends on many of the theoretical ideas we have explored in class, and these tools are also useful across a range of applications in both prediction. "],["introduction.html", "Introduction", " Introduction Linear regression is one of the most important tools in statistics, useful for both prediction and causal inference across a range of disciplines. At its core, regression is about using observed data to estimate the unknown true relationship between variables. Intuitively, this suggests two key features we might desire in a regression model: Low Bias: The model predicts the true relationship correctly on average Low Variance: The predictions do not vary widely These two features are illustrated nicely in the following figure: Ideally, a regression model would satisfy both of these conditions, and the estimates would look something like the bulls-eye on the top left. In many cases, however, there is a tradeoff between low bias and low variance. Ordinary least squares, the most commonly used regression model, does well on the first of these two criteria–under reasonable assumptions, it correctly predicts the true relationship on average. However, in cases with many explanatory variables, it fails the second criteria–even though the estimates are correct on average they may vary wildly depending on the specific sample of data that is used. In such cases, the OLS estimates will look like the bulls-eye on the top right in the figure above. In this project we consider a modification of Ordinary Least Squares, called regularization, that attempts to overcome this limitation. By penalizing large coefficients, this approach trades off unbiasedness (meaning that the model no longer makes the correct prediction on average) for less variance (meaning that a given estimate may be closer to the true value). Thus, penalized regression estimates may look more like the bulls-eye on the bottom left. We begin by introducing penalized regression models in section 2 and present derivations of their bias and variance in section 3. Next, we discuss the pros and cons of different types of penalized regression models in section 4. Then, in section 5, we illustrate the properties derived in section 3 on simulated data, showing how penalized regression estimates differ from OLS and how different types of regularization models differ from each other. Finally, in section 5, we briefly discuss interesting real world applications of penalized regression. "],["bias-and-variance-of-penalized-regression.html", "Bias and Variance of Penalized Regression 0.1 Assumptions 0.2 Bias 0.3 Variance", " Bias and Variance of Penalized Regression After introducing penalized regression models and finding the estimator for \\(\\hat{\\beta}\\), we now turn to the properties of these models. In particular, we derive the bias and variance of the estimators, given our focus on the trade-off between them, formally showing that penalized regression models are biased, but have lower variance than OLS. For simplicity, we work exclusively with ridge regression in these derivations, given that it is the only penalized regression model with a closed form solution, but numerical solutions for LASSO and elastic net show similar properties, as we show through simulation later on. 0.1 Assumptions We begin with the standard assumptions regarding our error terms: \\(E[\\epsilon \\mid X]=0\\) \\(Var[\\epsilon |X]=\\sigma^2 I_n\\) Error terms are uncorrelated 0.2 Bias First, we derive the bias of the ridge estimator. Using the fact that \\(\\hat{B}_{ridge}=(X^TX+ \\lambda I)^{-1}X^Ty\\), we have: \\[ \\begin{aligned} E\\left[\\hat{B}_{ridge}\\mid X \\right]-\\beta&amp;=E\\left[(X^TX+ \\lambda I)^{-1}X^Ty \\mid X \\right]-\\beta\\\\ &amp;= (X^TX+ \\lambda I)^{-1}X^TE\\left[X\\beta+\\epsilon \\mid X \\right]-\\beta \\text{ by the linearity of expected value}\\\\ &amp;= (X^TX+ \\lambda I)^{-1}X^TX\\beta-\\beta \\text{ since the error term has mean 0}\\\\ \\end{aligned} \\] Thus, the ridge estimator is unbiased only in the case that \\((X^TX+ \\lambda I)^{-1}X^TX=I\\), which occurs when \\(\\lambda=0\\) (that is, when we just have the OLS estimator and no penalty). When \\(\\lambda&gt;0\\), on the other hand, the ridge estimator is biased. This result is intuitive: since the OLS estimator is unbiased, we are introducing bias into our estimator by penalizing large coefficients. As we can see, the larger the penalty, the more biased our estimator will be. 0.3 Variance Now, we derive the variance of the ridge estimator. To do so, we will write the ridge estimator as a function of the OLS estimator, using the fact that \\(Var[\\hat{\\beta}_{OLS} \\mid X]= \\sigma^2 (X^T X)^{-1}\\). First, notice that we can write the \\(\\hat{B}_{ridge}\\) as a function of \\(\\hat{B}_{OLS}\\) \\[ \\begin{aligned} \\hat{B}_{ridge}&amp;=(X^TX+ \\lambda I)^{-1}X^Ty\\\\ &amp;=(X^TX+ \\lambda I)^{-1}X^TX(X^TX)^{-1}X^T y \\text{ since } X^TX(X^TX)^{-1}=I\\\\ &amp; = (X^TX+ \\lambda I)^{-1} X^TX \\hat{B}_{OLS} \\text{ since } \\hat{B}_{OLS}=(X^TX)^{-1}X^T y \\end{aligned} \\] Now, we can derive the variance, using the fact that \\(Var[\\hat{\\beta}_{OLS} \\mid X]= \\sigma^2 (X^T X)^{-1}\\). We have: \\[ \\begin{aligned} Var[\\hat{B}_{ridge} \\mid X]&amp;=Var\\left[(X^TX+ \\lambda I)^{-1} X^TX \\hat{B}_{OLS}\\right]\\\\ &amp;= (X^TX+ \\lambda I)^{-1}X^TX X^TVar[\\hat{B}_{OLS} \\mid X]\\left(\\left(X^TX+ \\lambda I\\right)^{-1}X^TX X^T\\right)^{T}\\\\ &amp;= (X^TX+ \\lambda I)^{-1}X^TX X^T\\sigma^2 (X^TX)^{-1}\\left(\\left(X^TX+ \\lambda I\\right)^{-1}X^TX X^T\\right)^{T} \\text{ since } Var[\\hat{\\beta}_{OLS} \\mid X]= \\sigma^2 (X^T X)^{-1}\\\\ &amp;= \\sigma^2 (X^TX+\\lambda I)^{-1}X^TX\\left(\\left(X^TX+ \\lambda I\\right)^{-1}\\right)^{T} \\end{aligned} \\] Now, we compare this variance to that of the OLS estimator: \\[ \\begin{aligned} Var(\\hat{\\beta}_{OLS})-Var(\\hat{\\beta}_{ridge})&amp;=\\sigma^2 (X^T X)^{-1}-\\sigma^2 (X^TX+\\lambda I)^{-1}X^TX\\left(\\left(X^TX+ \\lambda I\\right)^{-1}\\right)^{T}\\\\ &amp;= \\sigma^2 \\left((X^T X)^{-1}- (X^TX+\\lambda I)^{-1}X^TX\\left(\\left(X^TX+ \\lambda I\\right)^{-1}\\right)^{T} \\right)\\\\ &amp;= \\sigma^2 (X^TX+\\lambda I)^{-1} (2\\lambda X^TX)^{-2} +\\lambda^2 (X^TX)^{-3})(X^TX+ \\lambda I)^{-1})^{T}\\\\ &amp;= \\sigma^2 (X^TX+\\lambda I)^{-1}(2\\lambda I + \\lambda^2(X^TX))((X^TX+\\lambda I)^{_1})^T \\end{aligned} \\] Since \\((X^TX+\\lambda I)^{-1}\\), \\((2\\lambda I + \\lambda^2(X^TX))\\), and \\(((X^TX+\\lambda I)^{_1})^T\\) are all non-negative definite, this means that the variance of the OLS estimator is greater than that of the ridge estimator, as expected. "]]
