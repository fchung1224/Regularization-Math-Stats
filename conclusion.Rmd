# Conclusion 

In this project, we consider the theory and application of penalized regression models. We motivate our study by the bias-variance traeoff-- in some cases, it may be preferable to sacrifice a degree of unbiasedness in order to have an estimator with less variance. 

We begin by introducing penalized regression models and deriving the ridge regression estimator. Next, we prove analytically that penalized regression models are biased, but have less variance than OLS regression. We complement this theoretical work by simulation, showing empirically that penalized regression models can have lower mean squared error than OLS. Finally, we consider several interesting real worl applications of penalized regression. 

Given our simulation values, we are provided with very interesting findings. When we have multicolinear datasets, we see that OLS will provide the highest Coefficient Bias which is unlike what we had expected. From our simulations, we found that for our simulation specifications, Ridge Regression seemed to have the lowest bias, variance, and mse. Although it is hard to pinpoint to one specific reason, we are certain that given the multicolinearity between the X values, penalized regression models that shrink coefficients when there is multicolinearity help in reducing the bias and variance of the estimates. Initally based on the fact that we had some betas to equal zero, we expected LASSO regression to be the best in reducing the MSE values. But because there are a high number of significant parameters, as only 2 parameters are zero, Ridge Regression works better as most predictors impact the response. 