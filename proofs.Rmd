---
title: "Proof"
author: "Federico Chung"
date: "3/5/2021"
output: html_document
---

# Ridge Regression

$$
y_i = \bar{x}^T_i \beta
$$
Lets assume that we have predictors that range from $\bar{x}^T_i  = 1,x^2_i,x^3_i,...,x_i^n$

$\beta$ includes all the coefficients for all our predictors. 

$$
\begin{aligned}
\beta^T = [\beta_0, \beta_1,..., \beta_n]  \\
J = \sum^m_{i=0} (x_i^T\beta - y_i)^2 + \lambda ||\beta||^2
\end{aligned}
$$

J is our cost function, the value that we are trying to minimize. Normally we only reduce the sum of squared residuals $\sum^m_{i=0} (x_i^T\beta - y_i)^2 $ but now we also include our Shrinkage parameter $\lambda||\beta||^2$, or also called $L_2$ regularization

$$
\begin{aligned}
X^T = [\bar{X}^T_1, \bar{X}^T_2, \bar{X}^T_3, \bar{X}^T_4, ...,\bar{X}^T_n] \\
Y^T = [y_1, y_2, y_3,y_4,...,y_n] \\ 
J = (X\beta - Y)^T(X\beta - Y) + \lambda \beta^T\beta \\
\end{aligned}
$$

$(X\beta - Y)^T(X\beta - Y)$ --> a way to express squared values in linear algebra lingo

We try to minimize J so we use its derivative and set it to zero to see what value of $\beta$ will minimize the new cost function J

$$
\begin{aligned}
\frac{dJ}{d\beta} = 2X^T(X\beta - Y)+ \lambda(2\beta)\\
2X^T(X\beta - Y)+ \lambda(2\beta) \overset{set}{=} 0 \\
X^T(X\beta - Y)+ \lambda(\beta) = 0 \\
X^TX\beta - X^TY + \lambda\beta = 0 \\
X^TX\beta + \lambda\beta = X^TY\\
(X^TX+ \lambda I)\beta = X^TY\\
\beta = (X^TX+ \lambda I)^{-1}X^TY \\
\end{aligned}
$$
If $\lambda = 0$ we have a normal OLS model where:

$$
\beta = (X^TX)^{-1}X^TY \\
$$


Ridge estimates are a scaled version of the least squares estimates where $\hat{\beta}^{ridge} = \hat{\beta}/(1+\lambda)$

#Bias Variance Trade-off



The expected error, the mean squared error, or $E[(y - \hat{f}(x))^2]$ of our predictions can also be expressed as 

$$
\begin{aligned}
E[(y - \hat{f}(x))^2] = Bias[\hat{f}(x)]^2 + Var[\hat{f}(x)] + \sigma^2
\end{aligned}
$$


There are three components that affect the expected error of our predictions:

1. Bias

2. Variance

3. "Irreducible Error term" $\sigma^2$

It is possible to reduce expected error by increasing bias and reducing variance by a lot. Which is why we use ridge regression and lasso to increase the bias by significantly reducing the variance. The trade-off is an important way to reduce our mean squared error for values outside our sample. We tune our $\lambda$ parameter using cross-validation and see how it reacts when there is "new" data. 


Proof of the Bias Variance Trade-off

we are comparing our estimate of $\hat{y}$ with the true value of y. 

$$
\begin{aligned}
MSE(\hat{y}) = E[(y - \hat{y})^2] &= E(y^2 - 2y\hat{y} + \hat{y}^2)\\
&= E(y^2) - 2E(y)E(y^2) + E(\hat{y}^2) \\
&= Var(y) + E(y)^2 - 2E(y)E(\hat{y}) + E(\hat{y}^2)\\
&= Var(y) + y^2 - 2yE(\hat{y}) + E(\hat{y}^2) \\
& = Var(y) + E(\hat{y}^2)- 2yE(\hat{y})+ y^2 \\
&= Var(y) + (E(\hat{y}) - y)^2
\end{aligned}
$$





