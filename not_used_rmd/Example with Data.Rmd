---
title: "Example with Data"
author: "Lucas Leiter"
date: "3/7/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
install.packages("glmnet")
library(glmnet)
```

# Penalized Regression Simulation

Now that we have established the properties of different penalized regression models, we can illustrate these properties using simulated data. We will compare these estimators to OLS to show the tradeoff between bias and variance that we have previously proved. 

## Simulated Data


```{r}
set.seed(42)

n<-1000
p<-5000
real_p<-15

x<-matrix(rnorm(n*p), nrow=n, ncol=p)
y<- apply(x[,1:real_p], 1, sum) +rnorm(n)

train_rows<- sample(1:n, .66*n)
x.train <-x[train_rows, ]
x.test <-x[-train_rows, ]

y.train<-y[train_rows]
y.test<-y[-train_rows]



```


## Ridge:

```{r}
# Training Data:
alpha0.fit <-cv.glmnet(x.train, y.train, type.measure = "mse", alpha=0, family="gaussian")

alpha0.predicted <- predict(alpha0.fit, s=alpha0.fit$lambda.1se, newx=x.test)

mean((y.test -alpha0.predicted)^2)
```

## Lasso:

```{r}
alpha1.fit <-cv.glmnet(x.train, y.train, type.measure = "mse", alpha=1, family="gaussian")

alpha1.predicted <- predict(alpha1.fit, s=alpha1.fit$lambda.1se, newx=x.test)

mean((y.test -alpha1.predicted)^2)
```

