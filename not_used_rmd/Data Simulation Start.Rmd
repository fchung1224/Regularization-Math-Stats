---
title: "Data Simulation"
author: "Federico Chung"
date: "3/8/2021"
output: html_document
---

```{r setup, include=FALSE}
library(dplyr)
library(caret)
library(tibble)
library(MASS) 
library(Metrics)
```

We start a simulation of the data

In here we will use the train dataset to tune the right value for $\lambda$ and/or $\alpha$ depending on the model we are planning to use. Based on these estimates for $\lambda$ and $\alpha$ we are going to simulate the models using resampled results from the train dataset to see how much the values of beta vary depending on the model we use.

Each of the models tested, outside of OLS, have a different loss function through the inclusion of additional "penalties". The models we will be testing are the following:

-OLS

Which tries to find $\beta$ estimates that minimize:

$$
\begin{aligned}
MSE = \frac{1}{N}(Y - X\beta)^T(Y-X\beta)
\end{aligned}
$$

-Ridge Regression


Which has an additional regression penalty, called L2: $\lambda \beta^T\beta$

Therefore Ridge Regression tries to find $\beta$ estimates that minimize:s


$$
\begin{aligned}
&= MSE  +  \lambda \beta^T\beta\\
&= \frac{1}{N}(Y - X\beta)^T(Y-X\beta) + \lambda \beta^T\beta
\end{aligned}
$$
-LASSO

Which has an additional regression penalty, called L1: $\lambda|\beta|$

$$
\begin{aligned}
&= MSE  +  \lambda|\beta|\\
&= \frac{1}{N}(Y - X\beta)^T(Y-X\beta) +  \lambda|\beta|
\end{aligned}
$$

-Elastic Net

Which is a combination of both L1 and L2 penalties: $\lambda \beta^T\beta+\lambda|\beta| $

$$
\begin{aligned}
&= MSE  +  \lambda \beta^T\beta+\lambda|\beta|\\
&= \frac{1}{N}(Y - X\beta)^T(Y-X\beta) + \lambda_1 \beta^T\beta+\lambda_2|\beta|
\end{aligned}
$$
# Simulation

To start the simulation we created a predetermined set of betas, or the true coefficient estimates of beta

We then use the MVR Norm function which allows us to create X variables with a specified Covarience Matrix.

This allows our X-values to be

1. Normally Distributed explanatory variables
2. Multicollinear between the explanatory variables

The reason for the use of this simulation is that we wanted to simulate real world data where we tend to experience mutlicollinearity between the explanatory variables. Also we wanted to see if including multicollinearity our coefficient estimates for ridge, lasso, or elastic net can become an improvement to the estimates of the OLS model. 

The true values of Y will be estimated using the following formula:

$$
Y = X\beta + \epsilon 
$$
Where $\epsilon \sim N(0,25)$

```{r}
#simulation
set.seed(19873)
n <- 100    # Number of observations
p <- 50     # Number of predictors included in model
CovMatrix <- outer(1:p, 1:p, function(x,y) {.7^abs(x-y)})
x <- mvrnorm(n, rep(0,p), CovMatrix)
betas<- rnorm(p)
e <- rnorm(n, 0, sd = 25)

y <- x %*% betas + e

# Split data into train and test sets
train_rows <- sample(1:n, .66*n)
x.train <- x[train_rows, ]
x.test <- x[-train_rows, ]

y.train <- y[train_rows]
y.test <- y[-train_rows]

train <-data.frame(y.train,x.train)
colnames(train)[1]<-"y"

test<- data.frame(y.test,x.test)
colnames(test)[1]<-"y"

```

# Testing the Models


```{r}
## OLS
set.seed(45)
ols_model<-train(
  y~.,
  data=train, 
  method="lm",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("center", "scale"),
  metric = "RMSE"
)
```


```{r}
## Ridge
lambda_grid<-10^seq(-3, 4, length.out = 100)
# Set the seed
set.seed(253)

# Perform Ridge
ridge_model <- train(
  y~.,
  data = train,
  method = "glmnet",
  trControl = trainControl(method = "cv", number = 10, selectionFunction = "best"),
  preProcess = c("center", "scale"),
  tuneGrid = data.frame(alpha = 0, lambda = lambda_grid),
  metric = "RMSE",
  na.action = na.omit
)

```




```{r}
ridge_model$resample
ridge_model$bestTune

ridge_model$resample%>%
  summarize(cv_rRRMSE=mean(RMSE))
```


#LASSO
```{r}
lambda_grid<-10^seq(-3, 4, length.out = 100)
# Set the seed
set.seed(253)

# Perform Ridge
lasso_model <- train(
  y~.,
  data = train,
  method = "glmnet",
  preProcess = c("center","scale"),
  trControl = trainControl(method = "cv", number = 10, selectionFunction = "best"),
  tuneGrid = data.frame(alpha = 1, lambda = lambda_grid),
  metric = "RMSE",
  na.action = na.omit
)


```




# Elastic Net
```{r}
lambda_grid <- 10^seq(-3, 4, length.out = 100)
alpha_grid <- 10^seq(-3, 0, length.out = 50)

srchGrd = expand.grid(.alpha = alpha_grid, .lambda = lambda_grid)

# Set the seed
set.seed(253)

# Perform Elastic Net
elastic_net_model <- train(
  y~.,
  data = train,
  method = "glmnet",  
  preProcess = c("center","scale"),
  trControl = trainControl(method = "cv", number = 10, selectionFunction = "best"),
  tuneGrid = data.frame(alpha = .5, lambda = lambda_grid),
  metric = "RMSE",
  na.action = na.omit
)


```

#RESULTS (in sample)




```{r}
ols_mse<-ols_model$resample%>%
  summarize(cv_MSE=mean(RMSE^2))%>%
  mutate(Model = "OLS")

ridge_mse<-ridge_model$resample%>%
  summarize(cv_MSE=mean(RMSE^2))%>%
  mutate(Model = "RIDGE")

lasso_mse<-lasso_model$resample%>%
  summarize(cv_MSE=mean(RMSE^2))%>%
  mutate(Model =  "LASSO")

elastic_net_mse<-elastic_net_model$resample%>%
  summarize(cv_MSE=mean(RMSE^2))%>%
  mutate(Model =  "ELASTIC NET")

elastic_net_model$resample

mse<- rbind(ols_mse,ridge_mse,lasso_mse,elastic_net_mse)
mse

elastic_net_mse
```


#graphs

#RIDGE

```{r}
# Plot coefficients for each Ridge
plot(ridge_model$finalModel, xvar = "lambda", label = TRUE, col = rainbow(500))
```

```{r}
summary(ridge_model$bestTune)
```


```{r}
plot(ridge_model)
```


```{r}
plot(ridge_model, xlim = c(0,1000), ylim = c(25,30))
```

#LASSO


```{r}
# Plot coefficients for each Lasso
plot(lasso_model$finalModel, xvar = "lambda", label = TRUE, col = rainbow(500))
```

```{r}
plot(lasso_model, xlim = c(0,40), ylim = c(0,30))
```

```{r}
lasso_model$resample

lasso_model$bestTune

lasso_model$resample%>%
  summarize(cv_RMSE=mean(RMSE))
```

#ELASTIC NET
```{r}
# Plot coefficients for each Elastic Net
plot(elastic_net_model$finalModel, xvar = "lambda", label = TRUE, col = rainbow(500))
```




```{r}
alpha_grid<- c(0.001, 1, 0, 0.5)

srchGrd = expand.grid(.alpha = alpha_grid, .lambda = lambda_grid)

elastic_net_model_plot <- train(
  y~.,
  data = train,
  method = "glmnet",
  trControl = trainControl(method = "cv", number = 10, selectionFunction = "best"),
  tuneGrid = srchGrd,
  preProcess = c("center","scale"),
  metric = "rRMSE",
  na.action = na.omit
)

plot(elastic_net_model_plot,  xlim = c(0,1000), ylim = c(22,24) )

```

```{r}

plot(elastic_net_model_plot, xlim = c(0,200), ylim = c(22,24) )
```


```{r}
elastic_net_model$resample
elastic_net_model$bestTune
```


```{r}
elastic_net_model_plot$bestTune
```


#Predictions for the coefficients

Models:
- ols_model
- ridge_model
- lasso_model
- elastic_net_model


```{r}

coef(ols_model$finalModel)
coef(ridge_model$finalModel, ridge_model$bestTune$lambda)
```


```{r}
ols_predictions<- predict(ols_model, transformed)

ols_betas<- as.data.frame(as.matrix(coef(ols_model$finalModel)))
ols_betas<-ols_betas[-1,]
```

```{r}
get_rRMSE(ols_predictions, truth)
#get_bias(, betas)
```


```{r}
betas
```


```{r}
bias<- coef(ols_model$finalModel) - betas

bias
```


```{r}
a<-as.data.frame(as.matrix(coef(lasso_model$finalModel)))
a<-a[-1,]
a
```



```{r}
betas_df<-as.data.frame(betas)

betas_df<-rownames_to_column(betas_df, "Coefficient")

betas_df_new <- data.frame(t(betas_df[-1]))
colnames(betas_df_new) <- betas_df[, 1]

betas_df_new
```

```{r}
mean_coef<-rownames_to_column(a, "Coefficient")

df2 <- data.frame(t(mean_coef[-1]))
colnames(df2) <- mean_coef[, 1]
```









