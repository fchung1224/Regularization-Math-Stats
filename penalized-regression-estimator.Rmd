# Penalized Regression Estimators

This section introduces penalized regression models and then derives the estimator for ridge regression. 

## Penalized Regression Models

In any type of linear regression, we assume that the relationship between $y$, our dependent variable, and $X$, a matrix of our independent variables, can be written as:

$$y_i=\beta^TX+\epsilon_i$$

where  $\beta$ is a vector of coefficients and $\epsilon_i$ is the random error component with mean zero. The goal is to estimate the true coefficient values of $\beta$. As seen in Stat 155,  OLS regression estimates $beta$ by minimizing the sum of squared errors:

$$\hat{\beta}_{OLS}=\text{argmin}_\beta \frac{1}{N}\sum_{i=1}^n (y_i-\beta^TX_i)^2.$$

In penalized regression, we modify this  process by adding a penalty term for large coefficient values. There are two common ways to do this. The first, called ridge regression, estimates $\beta$ by minimizing the following equation:

$$\hat{\beta}_{Ridge}=\text{argmin}_\beta \frac{1}{N}\sum_{i=1}^n (y_i-\beta^TX_i)^2+\lambda||\beta||_2^2$$

where $||\beta||_2^2=\sum_j B_j^2$ and the penalty is thus based on the squared 2-norm of the coefficient vector.  $\lambda$ represents the penalty parameter -- a larger value of $\lambda$ means that we penalized large coefficients more, while a $\lambda$ value of zero is just OLS. The second common method is called LASSO regression, and uses the following equation instead:


$$\hat{\beta}_{LASSO}=\text{argmin}_\beta \frac{1}{N}\sum_{i=1}^n (y_i-\beta^TX_i)^2+\lambda||\beta||_1$$

where $||\beta||_1\sum_{j \in J}|B_j|$. In LASSO, the penalty is based on the 1-norm  of the coefficient vector, rather than the 2-norm as in ridge. Again, $\lambda$ represents the penalization parameter, with larger values corresponding to more deviation from OLS.

While both LASSO and Ridge have pros and cons (as we show through simulation later on) only Ridge regression has a closed form solution. Thus, our theoretical derivations focus only on ridge regression, although the concepts apply to LASSO a well.


## Deriving the Ridge Estimator
Now, after inroducing the idea behind penalized regression models, we derive the estimator for ride regression. 

Assume that we have predictors that range from $\bar{x}^T_i  = 1,x^2_i,x^3_i,...,x_i^n$

$\beta$ includes all the coefficients for all our predictors. 

$$
\begin{aligned}
\beta^T = [\beta_0, \beta_1,..., \beta_n]  \\
J = \sum^m_{i=0} (x_i^T\beta - y_i)^2 + \lambda ||\beta||^2
\end{aligned}
$$

J is our cost function, the value that we are trying to minimize. Normally we only reduce the sum of squared residuals $\sum^m_{i=0} (x_i^T\beta - y_i)^2 $ but now we also include our Shrinkage parameter $\lambda||\beta||^2$, or also called $L_2$ regularization

$$
\begin{aligned}
X^T = [\bar{X}^T_1, \bar{X}^T_2, \bar{X}^T_3, \bar{X}^T_4, ...,\bar{X}^T_n] \\
Y^T = [y_1, y_2, y_3,y_4,...,y_n] \\ 
J = (X\beta - Y)^T(X\beta - Y) + \lambda \beta^T\beta \\
\end{aligned}
$$

$(X\beta - Y)^T(X\beta - Y)$ --> a way to express squared values in linear algebra lingo

We try to minimize J so we use its derivative and set it to zero to see what value of $\beta$ will minimize the new cost function J

$$
\begin{aligned}
\frac{dJ}{d\beta} = 2X^T(X\beta - Y)+ \lambda(2\beta)\\
2X^T(X\beta - Y)+ \lambda(2\beta) \overset{set}{=} 0 \\
X^T(X\beta - Y)+ \lambda(\beta) = 0 \\
X^TX\beta - X^TY + \lambda\beta = 0 \\
X^TX\beta + \lambda\beta = X^TY\\
(X^TX+ \lambda I)\beta = X^TY\\
\beta = (X^TX+ \lambda I)^{-1}X^TY \\
\end{aligned}
$$
If $\lambda = 0$ we have a normal OLS model where:

$$
\beta = (X^TX)^{-1}X^TY \\
$$

Thus, ridge estimates are a scaled version of the least squares estimates where $\hat{\beta}^{ridge} = \hat{\beta}/(1+\lambda)$



